# llm interview

## åŸºç¡€

1. cross entropy (ç”¨æ¥åº¦é‡ä¸¤ä¸ªæ¦‚çŽ‡åˆ†å¸ƒé—´çš„å·®å¼‚)
   $H(p, q) = -\sum_{x} p(x) \log(q(x))$; äº¤å‰ç†µåˆ»ç”»äº†ä¸¤ä¸ªæ¦‚çŽ‡åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œå€¼è¶Šå°ï¼Œä¸¤ä¸ªæ¦‚çŽ‡åˆ†å¸ƒè¶ŠæŽ¥è¿‘
   å¯¹äºŽäºŒåˆ†ç±»ï¼š$L=\frac{1}{N}\sum_{i}L_i =- \frac{1}{N} \sum_{i}[y_i \log(p_i) + (1-y_i)\log(1-p_i)]$; yè¡¨ç¤ºçœŸå®žåˆ†å¸ƒ
   å¯¹äºŽå¤šåˆ†ç±»ï¼š$L=\frac{1}{N}\sum_{i}L_i =- \frac{1}{N} \sum_{i} \sum_{c=1}^{M}y_{ic}\log(p_{ic})$; $y_{ic}$å–0æˆ–1ï¼Œæ ·æœ¬içš„ç±»åˆ«ç­‰äºŽcå–1
2. kl divergence
   $L(y_{pre}, y_{ture}) = y_{true} \log{\frac{y_{ture}}{y_{pre}}} = y_{true}(\log{y_{true}} - \log{y_{pre}})$
3. Precision, Recall
   TP(True Positive), TN(True Negative), FP(False Positive), FN(False Negative)
   Precision(å‡†ç¡®çŽ‡) = TP/(TP+FP)
   Recall(å¬å›žçŽ‡) = TP/(TP+FN)
   F1-score = 2 * Precision * Recall / (Precision + Recall)
4. å¤§æ¨¡åž‹ç¾éš¾é—å¿˜é—®é¢˜
   å®šä¹‰ï¼šå¤§æ¨¡åž‹å­¦ä¹ æ–°çŸ¥è¯†çš„è¿‡ç¨‹ä¼šè¿…é€Ÿç ´åä¹‹å‰èŽ·å¾—çš„ä¿¡æ¯ï¼Œè€Œå¯¼è‡´æ¨¡åž‹æ€§èƒ½åœ¨æ—§ä»»åŠ¡ä¸­æ€¥å‰§ä¸‹é™ã€‚

å¦‚ä½•é¿å…æ¨¡åž‹è¿‡æ‹Ÿåˆ

## åˆ†è¯

tokenizer çš„åˆ†è¯æ–¹æ³•
[tokenization](../notes/llm/tokenizer.md)

## æ¨¡åž‹ç»“æž„

1. attention
   $att=softmax(\frac{qk^T}{\sqrt{d}}) v$
   åˆå§‹çš„Attentionå¾ˆæŽ¥è¿‘one hotåˆ†å¸ƒï¼Œä¸é™¤ä»¥æ ¹å·dï¼Œä¼šé€ æˆæ¢¯åº¦æ¶ˆå¤±
   multi_head_attï¼šå¯ä»¥å­¦ä¹ åˆ°ä¸åŒçš„çŸ¥è¯†ï¼Œå¢žå¼ºè¡¨è¾¾èƒ½åŠ›
   
   æ—¶é—´å¤æ‚åº¦ï¼ˆ[seq_len, hidden_size] * [hidden_size, seq_len] = n^2dï¼‰-> O(n^3) -> ç”±äºŽ d << nï¼Œåˆ™ O(n^2)ã€‚
   è®¾åºåˆ—é•¿åº¦ä¸º lï¼Œæ³¨æ„åŠ›çš„è®¡ç®—å¤æ‚åº¦ä¸º O(l^2); ä¸ºäº†é™ä½Žå¤æ‚åº¦å¯ä»¥ä½¿ç”¨ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¤æ‚åº¦ä¸º O(wL)
   ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼šåœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶ï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ›æœºåˆ¶ï¼ˆSliding Window Attention, SWAï¼‰,çª—å£ä¸º wã€‚

   ```python
   batch_size, seq_len, d_model = 1, 512, 768
   x = torch.rand((batch_size, seq_len, d_model))
   query, key, value = nn.Linear(d_model, 3 * d_model)(x).chunk(3, dim=2)  # åˆ›å»ºqkvä¸‰ä¸ªä¸åŒçš„çŸ©é˜µ [1, 512, 768]
   attn_score = torch.matmul(query, key.transpose(-1, -2)) / (d_model ** 0.5)
   attn_weights = nn.Softmax(dim=-1)(attn_score)
   att = torch.matmul(attn_weights, value)
   output = nn.Linear(d_model, d_model)(att)
   ```

   åœ¨æŽ¨ç†æ—¶ï¼Œç”±äºŽæ˜¯é¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼Œåˆ™ä¸‹ä¸€ä¸ªstepçš„è¾“å…¥å°±åŒ…å«äº†ä¸Šä¸€ä¸ªstepçš„å†…å®¹ï¼Œåªæ˜¯æœ«å°¾å¤šäº†ä¸€ä¸ªtokenã€‚
   é‚£ä¹ˆä¸‹ä¸€ä¸ªstepçš„è®¡ç®—ä¹Ÿåº”è¯¥åŒ…å«ä¸Šä¸€ä¸ªstepçš„è®¡ç®—ï¼ŒäºŽæ˜¯ KV_Cache=[(k_0,v_0), (k_1,v_1), ...]ã€‚
   å¯¹äºŽè¾“å…¥é•¿åº¦ä¸º Sï¼Œå±‚æ•°ä¸º Lï¼Œhidden_sizeä¸º d çš„æ¨¡åž‹ï¼Œéœ€è¦ç¼“å­˜çš„å‚æ•°é‡ä¸ºï¼š$2*s*d*L$
   ä»¥llama 7Bä¸ºä¾‹ï¼šL=32, hidden_size=4096, s=1kæ—¶ï¼š2*1024*4096*32=268435456 (512M)

   ```python
   # 1. multi head attention (KV Cache ä»¥ç©ºé—´æ¢æ—¶é—´)
   # è¾“å…¥åˆ†åˆ«ç»è¿‡W_qã€W_kã€W_vçš„å˜æ¢ä¹‹åŽï¼Œéƒ½åˆ‡æˆäº† num_head ä»½ï¼Œç»´åº¦ä¹Ÿä»Ž d_model é™åˆ°äº†d_headï¼Œå†åˆ†åˆ«å¯¹æ¯ä¸ªheadè¿›è¡Œattentionè®¡ç®—å¹¶æ‹¼æŽ¥
   qkw_1 = nn.Linear(d_model, 3*d_model)
   q_1, k_1, v_1 = qkv(x).chunk(3, dim=2)

   # 2. multi query attention (å‡å°‘ç©ºé—´æ¶ˆè€—ï¼Œé™ä½Žæ€§èƒ½)
   # ç»è¿‡W_qã€W_kã€W_vçš„å˜æ¢åŽåªå¯¹ Q è¿›è¡Œåˆ‡åˆ†ï¼Œè€Œ Kã€Vç›´æŽ¥åœ¨çº¿æ€§å˜æ¢çš„æ—¶å€™æŠŠç»´åº¦é™åˆ°äº†d_headï¼Œç„¶åŽè¿™nä¸ªQueryå¤´åˆ†åˆ«å’ŒåŒä¸€ä»½Kã€Vè¿›è¡Œattentionè®¡ç®—ï¼Œä¹‹åŽæŠŠç»“æžœæ‹¼æŽ¥èµ·æ¥ã€‚
   qkw_2 = nn.Linear(d_model, d_model + 2*d_head)  # d_model = num_head * d_head
   q_2, k_2, v_2 = qkv(x).split([d_model, d_head, d_head], dim=2)

   # 3. Group Query Attention (æŠ˜ä¸­æ–¹æ¡ˆ llama2,3)
   # ç»è¿‡W_qã€W_kã€W_vçš„å˜æ¢åŽ Q ä»ä¸å˜ï¼Œè€Œ Kã€Våœ¨çº¿æ€§å˜æ¢çš„æ—¶å€™æŠŠç»´åº¦é™åˆ°äº†group*d_headï¼ŒåŒä¸€ä¸ªgroupå†…çš„ Q å…±äº«åŒä¸€å¥—Kã€Vï¼Œä¸åŒgroupçš„Qæ‰€ç”¨çš„Kã€Vä¸åŒ
   group_head = num_head / group
   qkw_3 = nn.Linear(d_model, d_model + 2*group_head*d_head)
   q_3, k_3, v_3 = qkv(x).split([d_model, group_head*d_head, group_head*d_head], dim=2)
   ```
3. transformer (encoder-decoder)
   Loss: CrossEntropyLoss äº¤å‰ç†µæŸå¤±

   ç»“æž„ï¼š(n_layers=6, att_head=8, hidden=512, mlp_hidden=4*hidden, seq_len=256)
   tokenization: SentencePiece + BPE
   position: $PE(pos, 2i)=sin(pos / 10000^{2i / d}); PE(pos, 2i+1)=cos(pos / 10000^{2i / d})$
   embedding: token_embed + position_embed
   activation: ReLU
   normalization: layernorm $w * (x - \bar{x}) / (s + \epsilon) + b$

   6 * EncoderBlock (=> norm(x + dropout(multi_head_att(x))) -> norm(x + dropout(mlp(x))))  åŽnorm ==> en_out
   6 * DecoderBlock (=> norm(x + dropout(multi_head_att(x))) -> norm(x + dropout(cross_att(x, en_out, en_out))) -> norm(x + dropout(mlp(x))))  åŽnorm -> linear
   multi_head_att: x -> q, k, v -> att -> linear
   cross_multi_head_att: x -> q, en_out = k, en_out = v -> att -> linear
4. Bert (only-encoder)
   åŒå‘transformeræ¨¡åž‹ï¼š$P(w_i | w_1, \cdots, w_{i-1}, w_{i+1}, \cdots, w_n)$
   pre-training: (task_1: mask_lm(éšæœºmask 15%å¹¶é¢„æµ‹ï¼Œvocabç±»)ï¼›
   task_2: next_sentence_predict(è¾“å…¥ABä¸¤ä¸ªå¥å­ï¼Œåˆ¤æ–­Bæ˜¯ä¸æ˜¯Açš„ä¸‹ä¸€å¥))
   fine-Tuning: åˆ†ç±»(è¾“å…¥ABï¼Œåˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦å…·æœ‰ç›¸å…³æ€§)
   Lossï¼šNegative Log Likelihood è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤± $-\sum_1^n{\log{p(x_i; \theta)}}$

   ç»“æž„ï¼š(n_layer=12, att_head=12, hidden=768, mlp_hidden=4*hidden, dropout=0.1, seq_len=512)
   tokenization: WordPiece
   position: $PE(pos, 2i)=sin(pos / 10000^{2i / d}); PE(pos, 2i+1)=cos(pos / 10000^{2i / d})$
   embedding: token_embed + position_embed + segment_embed(å¥å­æ‹¼æŽ¥: [cls]A[sep]B[sep] ==> 000111)
   activation: $GELU(x)=x/2 *(1 + tanh(\sqrt{(2 / \pi)}*(x + 0.44715x^3)) )$
   normalization: layernorm ($w * (x - \hat{x}) / (s + \epsilon) + b$

   12 * EncoderBlock (=> x + multi_head_att(norm(x)) -> x + mlp(norm(x)) -> dropout)  å…ˆnorm
   multi_head_att: x -> q, k, v -> att -> linear
5. GPT (only-decoder)
   å•å‘transformeræ¨¡åž‹ï¼š$P(w_i | w_{i-k}, \cdots, w_{i-1})$
   pre-training: æ ¹æ®ç¬¬ä¸€ä¸ªtokené¢„æµ‹åŽé¢çš„token; LMHead: linear(vocab)
   fine-tuning: nåˆ†ç±»é—®é¢˜; ClsHead: linear(vocab) -> linear(n)
   Loss: CrossEntropyLoss äº¤å‰ç†µæŸå¤± (fine-tuneæ—¶ï¼Œcls_loss + ratio * lm_loss, é˜²æ­¢ä¸‹æ¸¸ç²¾è°ƒæ—¶å‡ºçŽ°ç¾éš¾æ€§é—å¿˜é—®é¢˜)

   ç»“æž„ï¼š(n_layers=12, att_head=12, hidden=768, mlp_hidden=4*hidden, dropout=0.1, seq_len=512)
   tokenization: SentencePiece (gpt1); BPE (gpt2)
   position: nn.Embedding(0-512) (gpt1)
   embedding: token_embed + position_embed
   activation: $GELU(x)=x/2 *(1 + tanh(\sqrt{(2 / \pi)}*(x + 0.44715x^3)) )$
   normalization: layernorm ($w * (x - \hat{x}) / (s + \epsilon) + b$

   12 * DecoderBlock (=> norm(x + dropout(multi_head_att(x))) -> norm(x + dropout(mlp(x))))  gpt2å…ˆnorm
   mask_multi_head_att: x -> q, k, v -> mask_att -> linear
   mask_att: att_score = mask(att_score)

   gpt2ä¸­å¼•å…¥äº†past_key_value, é˜²æ­¢æ¨¡åž‹åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­é‡æ–°è®¡ç®—ä¸Šä¸€æ¬¡è¿­ä»£è®¡ç®—å¥½çš„ä¸Šä¸‹æ–‡å€¼ï¼›
   gpt3ä¸­å¼•å…¥äº†ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶å’Œè‡ªé€‚åº”æ³¨æ„åŠ›è·¨åº¦æ¥æé«˜è®¡ç®—æ•ˆçŽ‡å’Œé•¿è·ç¦»ä¾èµ–çš„å»ºæ¨¡èƒ½åŠ›
6. Llama (only-decoder)
   å•å‘transformeræ¨¡åž‹ï¼š$P(w_i | w_{i-k}, \cdots, w_{i-1})$
   pre-training: æ ¹æ®å‰é¢çš„tokené¢„æµ‹åŽä¸€ä¸ªtokenï¼Œ temperature > 0æ—¶ï¼Œsoftmax(logits/temperature)å¹¶é‡‡æ ·top_p
   fine-tuning: sft, instruction-tuning
   Loss: CrossEntropyLoss äº¤å‰ç†µæŸå¤±

   ç»“æž„ï¼š(n_layers=32, att_head=32, hidden=4096, mlp_hidden=4*hidden, seq_len=2048)  llama2: seq_len=4096
   tokenization: SentencePiece + BPE
   position: RoPE [æ—‹è½¬ä½ç½®ç¼–ç ](../notes/llm/position.md)
   embedding: token_embed + position_embed
   activation: $SiLU(x) = x * sigmoid(x)$
   normalization: (Root Mean Square) RMSNorm ($W * \frac{x}{\sqrt{\frac{1}{n} \sum_i^n{x_i^2} + \epsilon}}$)

   32 * DecoderBlock (=> x + multi_head_att(norm(x)) -> x + mlp(norm(x)))  å…ˆnorm
   mask_multi_head_att: x -> q, k, v -> rope(q, k) -> mask_att -> linear
   mask_att: att_score = mask(att_score)

### embedding-model:
bce-embedding (å¼€æº)
text-davinci (openai)

### æ¨¡åž‹å‚æ•°è®¡ç®—
vocab_size=V; hidden_size=H; intermediate_size=H'; layers=L

Embedding(VH) + L * [ATT(3HH + HH) + MLP(2HH' + H'H) + Norm(H + H)] + Output(HV)

### æ¨¡åž‹è¿ç®—é‡è®¡ç®— ï¼ˆæµ®ç‚¹è¿ç®—æ¬¡æ•° FloatingPoint Operations, FLOPï¼‰
model_parameter=P; batch_size=B; seq_len=S
è®­ç»ƒè¯å…ƒæ€»æ•°ð¶=ðµS; num_header=ð‘ï¼Œheader_dim=ð·ï¼Œä¸­é—´çŠ¶æ€ç»´åº¦ð»=ð‘ð·

çŸ©é˜µä¹˜ç§¯è¿ç®—ï¼š[m,n] * [n, p] = 2mnp
multi_head_att: Q, K, V [B, S, H]; å¤šå¤´è®¡ç®—æ—¶éœ€è¦æ‹†åˆ†: Q', K', V' [B, N, S, D]
    $Q'K'^T = 2(BNSD * BNDS) =  2BNSDS = 2BS^2ND$
    ç¼©æ”¾: BNS^2; Softmax: 3BNS^2(æŒ‡æ•°ï¼ŒåŠ å’Œï¼Œå½’ä¸€åŒ–ï¼Œéƒ½æ˜¯å…ƒç´ çº§æ“ä½œ); V': 2BS^2ND
ä¸€æ¬¡m_att: (4BS^2ND + 4BNS^2) * L = 4BS^2N(D+1)L = 4CSL(H+D)
å‰å‘+åå‘ï¼š3 * 4BS^2N(D+1) = 12BS^2N(D+1)  ï¼ˆtransformerä¸­åå‘ä¼ æ’­è®¡ç®—é‡çº¦ä¸ºå‰å‘çš„ä¸¤å€ï¼‰

çº¿æ€§å˜æ¢ï¼š

## peft

1. prompt tuning (sft)
   hard prompt: ç±»ä¼¼äºŽin-context-learningä¸­çš„few shot.
   soft prompt: æŠŠ Prompt çš„ç”Ÿæˆä½œä¸ºä¸€ä¸ªä»»åŠ¡è¿›è¡Œå­¦ä¹ ï¼Œç›¸å½“äºŽæŠŠäººå·¥è®¾è®¡ç¦»æ•£çš„ Prompt å˜æˆæ¨¡åž‹è‡ªå·±è¿›è¡Œå­¦ä¹ ã€å°è¯•ï¼ˆè¿žç»­ï¼‰

   Prompt Tuning: è®­ç»ƒä¸€ä¸ªPromptEmbeddingå±‚ï¼Œå°†äººå·¥è¾“å…¥æˆ–éšæœºçš„prompt templateè°ƒæ•´ä¸ºæ¨¡åž‹èƒ½å¤Ÿç†è§£çš„ prompt tokenã€‚
   æµç¨‹ï¼šfrozen llm, token_embedding = prompt_embedding + text_embedding (åŽŸå§‹æ¨¡åž‹çš„embeddingè¾“å‡º)
   åˆå§‹åŒ–ï¼šä»»åŠ¡ç›¸å…³çš„å®žä½“æ–‡æœ¬è¿›è¡Œtokenizeæ¥åˆå§‹åŒ– ï¼ˆ10-20 tokenï¼‰
2. prefix tuning (sft)
   ä¼ ç»Ÿçš„fine-tuningèŠ±è´¹è¾ƒå¤§ï¼Œä¸”ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡éœ€è¦å­˜å‚¨ä¸åŒçš„æ¨¡åž‹ï¼Œprefixåªéœ€è¦ä¿å­˜prefix layerå³å¯

   æµç¨‹ï¼šæ·»åŠ ä¸€ä¸ªprefix(embed+mlpæˆ–ç›´æŽ¥embed)ï¼Œè‡ªå›žå½’æ¨¡åž‹è¡¨ç¤ºä¸º [prefix;x;y]; encoder-decoderæ¨¡åž‹è¡¨ç¤ºä¸º [prefix;x;prefix';y]
   åˆå§‹åŒ–ï¼šEmbedding(num_virtual_tokens, token_dim) æ²¡æœ‰å®žé™…æ„ä¹‰
3. p-tuning
   é’ˆå¯¹encoder-decoderæ¨¡åž‹ï¼Œæ·»åŠ  MLP(LSTM(input_embed)) æ¨¡å—
4. lora
   llmåœ¨é¢„è®­ç»ƒåŽï¼Œè¶Šå¤§çš„æ¨¡åž‹æƒé‡çŸ©é˜µçš„ç§©è¶Šå°ï¼ŒäºŽæ˜¯å°†éœ€è¦fine-tuneçš„å‚æ•°çŸ©é˜µWå˜æˆä¸¤ä¸ªå°çŸ©é˜µçš„ä¹˜ç§¯ W=ABï¼Œå³ï¼š$W_0+\Delta W=W_0 +AB$

   æµç¨‹ï¼š
   åˆå§‹åŒ–ï¼šAï¼ˆé«˜æ–¯åˆ†å¸ƒï¼‰ï¼ŒBï¼ˆåˆå§‹åŒ–ä¸º0ï¼‰

## è®­ç»ƒ

### sft

è®­ç»ƒæ•°æ®é‡çº§ï¼šllama(1T); llama2(2T); llama3(15T)
è®­ç»ƒæ­¥æ•°: ä¸€èˆ¬3ä¸ªepoch
è¯„ä¼°æŒ‡æ ‡: ï¼Œè¿™äº›æŒ‡æ ‡å­˜åœ¨å“ªäº›é—®é¢˜

### rlhf

1. é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹:
   é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æ˜¯ä¸€ä¸ª4å…ƒç»„ $(S,A,P_{a},R_{a})$ï¼Œå…¶ä¸­ï¼š
   - Sæ˜¯çŠ¶æ€ç©ºé—´çš„é›†åˆ
   - Aæ˜¯åŠ¨ä½œçš„é›†åˆ
   - $P_{a}(s,s')=P(s_{t+1}=s'\mid s_{t}=s,a_{t}=a)$ æ˜¯ t æ—¶åˆ» s çŠ¶æ€ä¸‹çš„åŠ¨ä½œ a å¯¼è‡´ t+1 æ—¶åˆ»è¿›å…¥çŠ¶æ€ s' çš„æ¦‚çŽ‡
   - $R_{a}(s,s')$ çŠ¶æ€ s ç»è¿‡åŠ¨ä½œ a è½¬æ¢åˆ°çŠ¶æ€ s' åŽæ”¶åˆ°çš„å³æ—¶å¥–åŠ±ï¼ˆæˆ–é¢„æœŸçš„å³æ—¶å¥–åŠ±ï¼‰
   - ç­–ç•¥å‡½æ•° $\pi$ æ˜¯ä»ŽçŠ¶æ€ç©ºé—´ S åˆ°åŠ¨ä½œç©ºé—´ A çš„æ˜ å°„ã€‚

æœ‰äº†è§£éšé©¬å°”ç§‘å¤«é“¾å—ï¼Œç»†è¯´(ç»™å‡ºå…¬å¼é‚£ç§)
CRF

2. [RLHFæµç¨‹](../notes/llm/rlhf.md)
   policy: GPT; action_space: å…¨è¯è¡¨; observation_space: å…¨è¯è¡¨*seq_len; reward;

   step 1: query_tensor -> sft_model -> response_tensor
   step 2: query_tensor + response_tensor -> reward_model(å°) -> reward
   step 3:

   ```
   q_a -> reward_model(freeze)  -> score                          -| 
   q_a -> actor_model           -> log_probs -    |                +         -> PPO
                                                  |-> kl(log_probs || ref_log_probs)
   q_a -> ref_model(freeze)     -> ref_log_probs -|
   ```

   ref_modelæ˜¯å†»ç»“çš„sft_modelï¼Œå…¶ç›®çš„æ˜¯é˜²æ­¢actorè®­æ­ªã€‚

   ä¼˜åŒ–ç›®æ ‡:

ç›®æ ‡å…¬å¼ä¸­è¡°å‡å› å­çš„ä½œç”¨ï¼Œå–å¤§å–å°æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ
RLHFçš„ç›®æ ‡å…¬å¼å¯ä»¥åŠ å…¥ä»€ä¹ˆå…¶ä»–çš„é¡¹ï¼Ÿ
RLHFä¸­PPOç®—æ¯”çŽ‡ç›¸å¯¹ä»€ä¹ˆæ¥ç®—ï¼Ÿ
ä¸ºå•¥RLHFä¸­è¦ç”¨PPOï¼Ÿå’Œå…¶ä»–RLç®—æ³•çš„åŒºåˆ«ï¼Ÿ

3. Reward model
   æ•°æ®æ ¼å¼ï¼šRewardDataCollatorWithPadding

   ```
      input_ids_chosen: "question + good_answer"
      attention_mask_chosen`
      input_ids_rejected: "question + bad_answer"
      attention_mask_rejected
   ```

   model: ç±»åž‹(SEQ_CLS)å±žäºŽText classification (1åˆ†ç±»ï¼Œå³æ‰“åˆ†)
   loss: $-LogSigmoid(x) = -\log{(\frac{1}{1+e^{-x}})}$, å³ `-nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()`
   trickï¼šä½¿ç”¨å¤šä¸ªå¥–åŠ±æ¨¡åž‹çš„è¾“å‡ºï¼Œå¢žåŠ æ•°æ®åº¦é‡çš„ä¿¡æ¯æº
   Rewardå¤šç›®æ ‡ï¼šï¼Ÿï¼Ÿï¼Ÿ
4. PPO (Proximal Policy Optimization, è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–)

   åˆå§‹åŒ–policyå‚æ•° $\theta_0$ å’Œæƒ©ç½šé¡¹æƒå€¼ $\beta_0$ï¼Œkl-divergence $delta$
   for $k = 0, 1, 2, \cdots$ do:
   $\;\;\;\;$ åœ¨policy $\pi_k=\pi(\theta_k)$ ä¸Šæ”¶é›†ä¸€æ‰¹ç»éªŒæ•°æ® $D_k$
   $\;\;\;\;$ ä½¿ç”¨ä»»æ„çš„ä¼˜åŠ¿è¯„ä¼°ç®—æ³•è¯„ä¼°ä¼˜åŠ¿ $\hat{A_t^{\pi_k}}$
   $\;\;\;\;$ é€šè¿‡æ‰§è¡Œ K æ­¥minibatchæ¥è®¡ç®—policyæ›´æ–°ï¼š $\theta_{k+1} = \arg\max_{\theta} L_{\theta_k}(\theta) - \beta_k D_{kl}(\theta || \theta_k)$
   $\;\;\;\;$ if $D_{kl}(\theta_{k+1} || \theta_k) \leq 1.5\delta$ :
   $\;\;\;\;$ $\;\;\;\;$ $\beta_{k+1} = 2 \beta$
   $\;\;\;\;$ elif $D_{kl}(\theta_{k+1} || \theta_k) \geq \delta/1.5$:
   $\;\;\;\;$ $\;\;\;\;$ $\beta_{k+1} = \beta / 2$
5. DPO

## Prompt Engineering

1. Prompt Creator (æç¤ºè¯ç”Ÿæˆå™¨)

   å‡è®¾ä½ æ˜¯ä¸€ä¸ªprompt exportï¼Œæˆ‘æƒ³è®©chatgptç”¨pythonä»£ç å®žçŽ°ä¸€ä¸ªè®¡ç®—å™¨ï¼Œè¯·ç»™æˆ‘ä¸€ä¸ªå¥½çš„promptã€‚
2. Structured Promptï¼šè§’è‰² + ä»»åŠ¡ + è¦æ±‚ + æç¤º

   è§’è‰²ï¼šå‡è®¾ä½ æ˜¯ä¸€ä¸ªæœ‰ç€ä¸°å¯Œç»éªŒçš„pythonç¨‹åºå‘˜ã€‚
   ä»»åŠ¡ï¼šè¯·ç”¨pythonä»£ç ç»˜åˆ¶ä¸€ä¸ªäº”è§’æ˜Ÿã€‚
   è¦æ±‚ï¼šè¯·ä½¿ç”¨matplotlibè¿™ä¸ªåº“ï¼Œçº¿æ¡ä½¿ç”¨çº¢è‰²ã€‚
   æç¤ºï¼šäº”è§’æ˜Ÿéœ€è¦å…ˆè®¡ç®—äº”ä¸ªé¡¶ç‚¹ï¼Œç„¶åŽåœ¨é—´éš”ä¸€ä¸ªé¡¶ç‚¹çš„ä¸¤ä¸ªé¡¶ç‚¹ä¹‹é—´ä¸¤ä¸¤è¿›è¡Œè¿žçº¿ã€‚
3. One / Few Shot Prompt

   å°†è‹±è¯­ç¿»è¯‘ä¸ºæ±‰è¯­ï¼š
   big => å¤§
   small =>
4. COT (Chain of Thought)

   one-shot cot:
   Q: Rogeræœ‰5ä¸ªç½‘çƒã€‚ä»–åˆä¹°äº†ä¸¤ç½ç½‘çƒï¼Œæ¯ä¸ªç½å­æœ‰3ä¸ªç½‘çƒã€‚ä»–çŽ°åœ¨æœ‰å¤šå°‘ä¸ªç½‘çƒ?
   A: Rogerä¸€å¼€å§‹æœ‰5ä¸ªçƒã€‚2ç½3ä¸ªç½‘çƒï¼Œæ¯ç½ç­‰äºŽ6ä¸ªç½‘çƒã€‚5 + 6 = 11ã€‚ç­”æ¡ˆæ˜¯11ã€‚
   Q: é¤åŽ…æœ‰23ä¸ªè‹¹æžœã€‚å¦‚æžœä»–ä»¬ä½¿ç”¨äº†20ä¸ªè‹¹æžœåšåˆé¤ï¼Œåˆä¹°äº†6ä¸ªï¼Œä»–ä»¬è¿˜æœ‰å¤šå°‘ä¸ªè‹¹æžœ?

   zero-shot cot:
   é¤åŽ…æœ‰23ä¸ªè‹¹æžœã€‚å¦‚æžœä»–ä»¬ä½¿ç”¨äº†20ä¸ªè‹¹æžœåšåˆé¤ï¼Œåˆä¹°äº†6ä¸ªï¼Œä»–ä»¬è¿˜æœ‰å¤šå°‘ä¸ªè‹¹æžœ?

   è®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ€è€ƒ / è®©æˆ‘ä»¬é€æ­¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬å¾—åˆ°æ­£ç¡®çš„ç­”æ¡ˆ(ä¼˜å…ˆ)ã€‚
   (Let's think step by step / Let's work this out in a step by step way to be sure we have the right answer.)
5. ReACT (Reason+Act ååŒæ€è€ƒå’ŒåŠ¨ä½œ)

   ä¸€ç§reinforce language agentsï¼ŒæŒ‰ç…§ think -> act -> observation -> think... çš„æ¨¡å¼æ¥è§£å†³é—®é¢˜ã€‚å…¶ä¸­ï¼Œactå°±æ˜¯å’ŒçŽ¯å¢ƒäº¤äº’(å¦‚ï¼šæŸ¥è¯¢äº’è”ç½‘ï¼Œè°ƒç”¨å·¥å…·ï¼Œæ‰§è¡Œä»£ç ç­‰)ã€‚

   promptï¼šå°½ä½ æ‰€èƒ½å›žç­”ä»¥ä¸‹é—®é¢˜ã€‚æ‚¨å¯ä»¥è®¿é—®ä»¥ä¸‹å·¥å…·:\n\n{tools}\n\nä½¿ç”¨ä»¥ä¸‹æ ¼å¼:\n\nQuestion: æ‚¨å¿…é¡»å›žç­”çš„è¾“å…¥é—®é¢˜\nThought: ä½ åº”è¯¥ç»å¸¸æ€è€ƒè¦åšä»€ä¹ˆ\nAction: è¦é‡‡å–çš„è¡ŒåŠ¨ï¼Œåº”è¯¥æ˜¯ [{tool_names}] ä¸­ä¹‹ä¸€\nAction Input: åŠ¨ä½œçš„è¾“å…¥\nObservation: åŠ¨ä½œçš„ç»“æžœ\n... (å…¶ä¸­ Thought/Action/Action Input/Observation å¯ä»¥é‡å¤Næ¬¡)\nThought: æˆ‘çŽ°åœ¨çŸ¥é“æœ€åŽçš„ç­”æ¡ˆäº†\nFinal Answer: åŽŸå§‹è¾“å…¥é—®é¢˜çš„æœ€ç»ˆç­”æ¡ˆ\n\nBegin!\n\nQuestion: {input}\nThought:{agent_scratchpad}
6. Reflexion (å¤±è´¥åŽè‡ªæˆ‘åæ€)

   ä¸€ç§reinforce language agentsï¼ŒæŒ‰ç…§ task -> trajectory -> evaluation -> Reflection(å¦‚æžœå¤±è´¥åˆ™åæ€) -> next trajectory... çš„æ¨¡å¼æ¥è§£å†³é—®é¢˜ã€‚

## rag

langchainä¸­çš„æ¨¡å—:
chains, prompts, models, indexes, memory, agents

1. chains: é“¾å¼pipelineå’Œæ–‡æ¡£é“¾ï¼Œæ–‡æ¡£é“¾å¦‚ä¸‹ï¼š
   stuff: å°†æ‰€æœ‰æ–‡æ¡£ç»„æˆä¸€ä¸ªæ–‡æ¡£åˆ—è¡¨ï¼Œå…¨éƒ¨æ”¾åˆ°contextä¸­ï¼ˆé€‚ç”¨äºŽå°æ–‡æ¡£ï¼‰ï¼›
   refine: å¾ªçŽ¯éåŽ†æ¯ä¸€ä¸ªæ–‡æ¡£ï¼Œæ¯æ¬¡è¾“å…¥ä¸­é—´ç­”æ¡ˆï¼ˆä¸Šä¸€ä¸ªæ–‡æ¡£çš„ç­”æ¡ˆï¼‰å’Œä¸€ä¸ªæ–‡æ¡£ä½œä¸ºcontextï¼ˆé€‚ç”¨äºŽå¤§æ–‡æ¡£ï¼‰ï¼›
   map reduce: å¾ªçŽ¯éåŽ†æ¯ä¸€ä¸ªæ–‡æ¡£å¾—åˆ°è¾“å‡ºç»“æžœï¼Œå°†æ‰€æœ‰ç»“æžœç»„åˆæˆæ–°æ–‡æ¡£ä½œä¸ºè¾“å…¥ï¼›
   map re-rank: å¾ªçŽ¯éåŽ†æ¯ä¸€ä¸ªæ–‡æ¡£å¾—åˆ°è¾“å‡ºç»“æžœï¼Œå¹¶ç»™å‡ºæ¯ä¸ªç­”æ¡ˆçš„ç¡®å®šæ€§å¾—åˆ†ï¼Œè¿”å›žå¾—åˆ†æœ€é«˜çš„ä¸€ä¸ªã€‚
2. prompts: prompt templates
3. models: llms, chats, text_embedding
4. indexes: document_loaders, text_splitters, vectorstore, retrievers
   multi query retriever: æ ¹æ®queryç”Ÿæˆå¤šä¸ªé—®é¢˜ï¼Œå¹¶æ ¹æ®è¿™äº›é—®é¢˜æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼›
   contextual compression: åŽ‹ç¼©å•ä¸ªæ–‡æ¡£ï¼Œé¿å…è¿”å›žä¸å¿…è¦çš„å†…å®¹ï¼›
   ensemble retriever: ä½¿ç”¨å¤šä¸ªretrieverï¼Œæ ¹æ®ç®—æ³•å¯¹ç»“æžœè¿›è¡ŒæŽ’åºï¼Œè¿”å›žæ›´å¥½çš„ç»“æžœï¼›
   multi vector retriever: åœ¨å¤§æ®µæ–‡æ¡£ä¸­åˆ†å‰²å°æ®µæ–‡æ¡£ï¼Œæ£€ç´¢å°æ®µæ–‡æ¡£å¹¶å®šä½åˆ°å¤§æ®µæ–‡æ¡£ï¼›
   parent document retriever: æ£€ç´¢æ—¶ï¼Œå…ˆèŽ·å–å°å—æ–‡æ¡£ï¼Œå¹¶æ ¹æ®å®ƒæŸ¥æ‰¾çˆ¶æ–‡æ¡£ IDï¼Œå¹¶è¿”å›žé‚£äº›è¾ƒå¤§çš„æ–‡æ¡£ï¼›
5. memory: å†…å­˜ç®¡ç†ï¼ŒMessageHistoryï¼Œ buffer, KGMemory(çŸ¥è¯†å›¾è°±)...
6. agents: llm agent, multi agent ...

ragé‡‡ç”¨Top-kè¿›è¡Œå¬å›žï¼Œè¿™æ ·å­˜åœ¨æ£€ç´¢å‡ºæ¥çš„chunksä¸ä¸€å®šå®Œå…¨å’Œä¸Šä¸‹æ–‡ç›¸å…³ï¼Œæœ€åŽå¯¼è‡´å¤§æ¨¡åž‹è¾“å‡ºç»“æžœä¸ä½³ã€‚
rerank: å°†åŽŸæœ‰çš„Top-kå¬å›žï¼Œæ‰©å¤§å¬å›žæ•°é‡ï¼Œåœ¨å¼•å…¥ç²—æŽ’æ¨¡åž‹ï¼ˆpolicyï¼Œå°æ¨¡åž‹ï¼ŒLLMï¼‰ï¼Œå¯¹å¬å›žç»“æžœç»“åˆä¸Šä¸‹æ–‡è¿›è¡Œé‡æŽ’ï¼Œæ¥æ”¹è¿›ragæ•ˆæžœã€‚

langchainå®žçŽ°rag:

```python
# step 1: load pdf
from langchain.document_loaders import PyPDFLoader
loader = PyPDFLoader("https://arxiv.org/pdf/2309.10305.pdf")
pages = loader.load_and_split()

# step 2: split text
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
   chunk_size = 500,
   chunk_overlap = 50,)
docs = text_splitter.split_documents(pages)

# step 3: build vectorstore
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS  # local: faiss; online: pinecone

embed_model = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(
   documents=docs, embedding=embed_model , collection_name="openai_embed")

# step 4: retrieval 
query = "How large is the baichuan2 vocabulary?"
results = vectorstore.similarity_search(query, k=3)

# step 5: build prompt and model
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from langchain.chat_models import ChatOpenAI
source_knowledge = "\n".join([x.page_content for x in results])
augmented_prompt = f"""Using the contexts below, answer the query.
   contexts: {source_knowledge}
   query: {query}"""
messages = [
   SystemMessage(content="You are a helpful assistant."),
   HumanMessage(content=augmented_prompt), ]
chat = ChatOpenAI(
   openai_api_key="",
   model='gpt-3.5-turbo')
res = chat(messages)
```

## é—®é¢˜1ï¼š[context length](../notes/llm/position.md)

è§£å†³content lengthé•¿åº¦é—®é¢˜

## é—®é¢˜äºŒï¼š å¹»è§‰

å®šä¹‰ï¼šå¤§æ¨¡åž‹å›žç­”ä¸å‡†ç¡®ã€å‰åŽä¸ä¸€è‡´ç­‰é—®é¢˜ï¼Œç”Ÿæˆçš„å†…å®¹å¹¶éžåŸºäºŽè®­ç»ƒæ•°æ®æˆ–ä¸ç¬¦åˆäº‹å®žã€‚

åŽŸå› ï¼š

1. æ•°æ®è´¨é‡ï¼šè®­ç»ƒæ•°æ®çš„è´¨é‡ä¸è¶³ï¼Œå™ªå£°è¾ƒå¤šï¼Œä¼šå¯¼è‡´å‡ºçŽ°å¹»è§‰ï¼›æˆ–æ˜¯æŸä¸€ç±»æ•°æ®å¤§é‡é‡å¤å¯¼è‡´æ¨¡åž‹äº§ç”Ÿåå¥½ï¼›
2. è§£ç è¿‡ç¨‹ä¸­çš„éšæœºæ€§ï¼štop-k(beam search), top-p(æ ¸é‡‡æ ·), temperature(logits/T)ï¼›
   æ ¸é‡‡æ ·ï¼šç”±äºŽtop-kä¸­çš„kä¸å¥½ç¡®å®šï¼Œtop-påªä»Žç´¯ç§¯æ¦‚çŽ‡è¾¾åˆ°pçš„æœ€å°å•è¯é›†åˆä¸­é€‰æ‹©ä¸€ä¸ªå•è¯
   eg: 0.664, 0.199, 0.105...ï¼Œ p=0.9æ—¶åªä»Žå‰ä¸¤ä¸ªé‡‡æ ·
   temperature: æ¸©åº¦è¶Šå°å·®å¼‚è¶Šå¤§ï¼Œæ¸©åº¦è¶Šå¤§å·®å¼‚è¶Šå°
   ä½¿ç”¨çš„å…ˆåŽé¡ºåºæ˜¯ top-k -> top-p -> Temperature
3. æœ€å¤§ä¼¼ç„¶æ€§ç›®æ ‡ï¼šå¤§æ¨¡åž‹çš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–ä¸‹ä¸€ä¸ªtokençš„æ¦‚çŽ‡ï¼Œå› æ­¤ï¼Œæ¨¡åž‹æ›´çœ‹é‡çœ‹èµ·æ¥æ­£ç¡®ï¼Œè€Œä¸æ˜¯è¾“å‡ºå†…å®¹çš„æ­£ç¡®æ€§ï¼›
4. è®­ç»ƒæ•°æ®ä¸­æœ¬èº«ä¸åŒ…å«é—®é¢˜ç›¸å…³å†…å®¹ï¼ˆin-context learning è§£å†³ï¼‰ï¼›
5. ä¸Šä¸‹æ–‡ç†è§£ï¼šå¤§æ¨¡åž‹éœ€è¦ç†è§£ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥ç”Ÿæˆå‡†ç¡®çš„ç­”æ¡ˆï¼Œå¦‚æžœä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸è¶³æˆ–æ¨¡åž‹æ— æ³•æœ‰æ•ˆå¤„ç†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå°±ä¼šå¯¼è‡´æ¨¡åž‹æ— æ³•ç†è§£ä¸Šä¸‹æ–‡å«ä¹‰ï¼Œä»Žè€Œäº§ç”Ÿå¹»è§‰ã€‚

è§£å†³æ–¹æ³•ï¼š

1. æé«˜æ•°æ®è´¨é‡ï¼ˆåŒ…æ‹¬é¢„è®­ç»ƒæ•°æ®å’Œsftæ•°æ®ï¼‰ï¼›
2. é‡‡ç”¨æ›´é•¿æ›´å¥½çš„ä½ç½®ç¼–ç ï¼›
3. promptå·¥ç¨‹ï¼šé‡‡ç”¨æ›´åˆç†çš„promptï¼ˆå¦‚ï¼šcotï¼‰ï¼Œæˆ–agentï¼ˆå¦‚ï¼šReActï¼‰æˆ–è¦æ±‚å¤§æ¨¡åž‹ä¸ç¡®å®šçš„ä¸å›žç­”ï¼›
4. RAGå€ŸåŠ©å¤–éƒ¨çŸ¥è¯†ï¼Œä¸¥æ ¼æŒ‰ç…§ç»™å®šçŸ¥è¯†å›žç­”ï¼›
5. é›†æˆå­¦ä¹ ï¼šå°†å¤šä¸ªæ¨¡åž‹çš„é¢„æµ‹ç»“æžœè¿›è¡Œé›†æˆï¼Œä»¥æé«˜é¢„æµ‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

## é—®é¢˜ä¸‰ï¼šåŠ é€Ÿ

è®­ç»ƒåŠ é€Ÿï¼š[deepspeed](../notes/llm/deepspeed.md)
æŽ¨ç†åŠ é€Ÿï¼š

1. FlashAttentionï¼šé€šè¿‡çŸ©é˜µåˆ†å—è®¡ç®—ä»¥åŠå‡å°‘å†…å­˜è¯»å†™æ¬¡æ•°çš„æ–¹å¼ï¼Œæé«˜æ³¨æ„åŠ›åˆ†æ•°çš„è®¡ç®—æ•ˆçŽ‡ï¼›
2. PagedAttentionï¼šé’ˆå¯¹å¢žé‡è§£ç é˜¶æ®µï¼Œå¯¹äºŽ KV ç¼“å­˜è¿›è¡Œåˆ†å—å­˜å‚¨ï¼Œå¹¶ä¼˜åŒ–äº†è®¡ç®—æ–¹å¼ï¼Œå¢žå¤§äº†å¹¶è¡Œè®¡ç®—åº¦ï¼Œä»Žè€Œæé«˜äº†è®¡ç®—æ•ˆçŽ‡ï¼›
3. TGI (Text Generation Inference)

## é—®é¢˜å››ï¼šå›½äº§å¤§æ¨¡åž‹

é˜¿é‡Œï¼šé€šä¹‰åƒé—®
ç™¾åº¦ï¼šæ–‡å¿ƒä¸€è¨€
è…¾è®¯ï¼šhunyuan
ç§‘å¤§è®¯é£žï¼šè®¯é£žæ˜Ÿç«
å­—èŠ‚ï¼šäº‘é›€
ç™¾å·æ™ºèƒ½ï¼šbaichuan
æ™ºè°±AIï¼šGLM
æœˆä¹‹æš—é¢ï¼škimichat
ç¨€å®‡ç§‘æŠ€: minimax (moe)
å¹»æ–¹ï¼š deepseek

## é—®é¢˜äº”ï¼šæµ‹è¯„
MMLUï¼šç»¼åˆæ€§çš„å¤§è§„æ¨¡è¯„æµ‹æ•°æ®é›†ã€‚(å•é€‰ï¼Œå››é€‰ä¸€ï¼Œ[é—®é¢˜ï¼šxxx; é€‰é¡¹ï¼šxxxï¼›ç­”æ¡ˆï¼šA])ï¼›
BIG-Benchï¼šç»¼åˆè¯„æµ‹ä½“ç³»ã€‚(QAï¼Œå•é€‰)ï¼›
HELMï¼šä¸€ä¸ªå…¨é¢è€Œç³»ç»Ÿ çš„è¯„ä¼°ä½“ç³»ã€‚ï¼ˆQAï¼Œå•é€‰ï¼‰
C-Evalï¼šä¸€ä¸ªä¸“é—¨ä¸ºä¸­æ–‡å¤§è¯­è¨€æ¨¡åž‹è®¾è®¡çš„ç»¼åˆè¯„æµ‹ä½“ç³»ã€‚(å•é€‰)
CMMLUã€AGIEvalã€MMCUã€M3KEå’ŒXiezhiç­‰ã€‚ 


transformerç»´åº¦å˜åŒ–:

è®¾ seq_len = sï¼Œvocab_size=v åˆ™
embedding: (s, d_model)
q,k,v: (s, hidden_size)
q*k^T: (s, s)
attn: (s, hidden_size)
multi_head: (s, d_model)
MLP: (s, d_model) -> (s, d_imd) -> (s, d_model)
output: (s, v)
softmax: (s, v)