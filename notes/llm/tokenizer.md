# Tokenizer

## 1. Methodology
当前 tokenization 主要分为：word，sub-word， char-level 三个类型.

1. word: 分成单个字；
  缺点：
    1) vocabulary size太大；
    2) vocabulary 中存在较多相似的词；
    3) 面临严重的OOV（Out of vocabulary超出词汇表的词）问题。
2. sub-word：低频词拆分，高频词不拆分；如高频的boy不拆分，低频的boys拆分为boy和s；
   方法：Byte-Pair Encoding (BPE), WordPiece, Unigram, SentencePiece
3. char-level:

### 1.1 BPE--Byte Pair Encoding(llama, GPT)

1) 准备足够大的训练语料，并确定期望的Subword词表大小；
2) 将单词拆分为成最小单元。比如英文中26个字母加上各种符号，这些作为初始词表；
3) 在语料上统计单词内相邻单元对的频数，选取频数最高的单元对合并成新的Subword单元；
4) 重复第3步直到达到第1步设定的Subword词表大小或下一个最高频数为1.

```
for example:
    'low</w>': 5, 'lower</w>': 2, 'newest</w>': 6, 'widest</w>': 3
    1) l,o,w,e,r,n,s,t,i,d,</w>
    2) es出现6+3次，属于高频词需要合并 -> est -> est</w>; 此时词表为 l,o,w,r,n,i,d,est</w>
    3) 迭代至达到预设的词表大小，或最高词频出现频率为1
 ```

### 1.2 WordPiece(Bert)

WordPiece本质是同BPE，但在每次merge时，BPE选择频数最高的相邻子词合并，而WordPiece是选择能够最大化训练数据似然的合并。
设句子 $S=(t_1, t_2, \cdots, t_n)$ 由 $n$ 个子词组成，其中 $t_i$ 表示子词，且假设各个子词之间是独立存在的，则句子S的语言模型似然值等价于所有子词概率的乘积：

$$
\log P(S)=\sum_{i=1}^{n} \log P(t_i)
$$

假设把相邻位置的x和y两个子词进行合并，合并后产生的子词记为z，此时句子似然值的变化可表示为：

$$
\log P(t_z)-[\log P(t_x) + \log P(t_y)]=\log[\frac{P(t_z)}{P(t_x)P(t_y)}]
$$

### 1.3 ULM--Unigram Language Model()

先初始化一个大词表，根据评估准则不断丢弃词表，直到满足限定条件.
对于句子S，$x=(x_1, x_2, \cdots, x_n)$ 为句子的一个分词结果，由 $n$ 个子词组成。则当前分词下句子 $S$ 的似然值可以表示为：

$$
P(x)=\prod_{i=1}^{n} \log P(x_i)
$$

对于句子 $S$，选择似然值最大的作为分词结果，则可以表示为：

$$
x^*=\arg\max_{x\in U(x)} P(x)
$$

...

### 1.4 SentencePiece (T5, llama)
https://blog.51cto.com/u_16099302/8891834
上述三种方法都有一个前提：输入以空格来区分，但很多语言的词语无法使用空格进行区分（eg:中文），
把一个句子看作一个整体，再拆成片段（空格space也当作一种特殊字符来处理），再用BPE或者Unigram算法来构造词汇表。

eg: "I have a new GPU!" ==> ['_I', '_have', '_a', '_new', '_G', 'PU', '!'] 

## 2. comparison

### 2.1 BPE(Byte-Pair Encoding)与WordPiece

相似点：
1. 分词目标：WordPiece和BPE都旨在将文本分解为子词或字符级别的单位，以便更好地处理未登录词和稀有词，提高模型对复杂词汇和短语的处理能力。
2. 无监督学习：WordPiece和BPE都是无监督学习方法，不需要依赖外部的标注数据，而是通过分析输入文本自动构建词典。

不同点：
1. 拆分策略：WordPiece采用贪婪的自顶向下的拆分策略，将词汇表中的词分解为更小的子词。它使用最大似然估计来确定最佳的分割点，并通过词频来更新词典。BPE则采用自底向上的拆分策略，通过合并频率最高的词对来构建词典。它使用词频来选择合并的词对，并通过更新词频来更新词典。
2. 分割粒度：WordPiece通常将词分解为更小的子词，例如将"running"分解为"run"和"##ning"。这些子词通常以"##"前缀表示它们是一个词的一部分。BPE则将词分解为更小的子词或字符级别的单位。它不使用特殊的前缀或后缀来表示子词。
3. 处理未登录词：WordPiece和BPE在处理未登录词时有所不同。WordPiece通常将未登录词分解为更小的子词，以便模型可以更好地处理它们。而BPE则将未登录词作为单独的词处理，不进行进一步的拆分。
    总体而言，WordPiece和BPE都是有效的分词方法，选择使用哪种方法取决于具体的任务需求和语料特点。

### 2.2

##### 举例 介绍一下 不同 大模型LLMs 的分词方式？

大模型语言模型（Large Language Models，LLMs）通常采用不同的分词方式，这些方式可以根据任务和语料库的不同进行调整。以下是一些常见的大模型LLMs的分词方式的举例：

1. 基于词典的分词：这是最常见的分词方式之一，使用一个预先构建好的词典来将文本分解为单词。例如，BERT模型使用WordPiece分词器，将文本分解为词片段（subword units），并在词典中查找匹配的词片段。
2. 基于字符的分词：这种方式将文本分解为单个字符或者字符级别的单位。例如，GPT模型使用字节对编码（Byte Pair Encoding，BPE）算法，将文本分解为字符或字符片段。
3. 基于音节的分词：对于一些语言，特别是拼音文字系统，基于音节的分词方式更为常见。这种方式将文本分解为音节或音节级别的单位。例如，对于中文，可以使用基于音节的分词器将文本分解为音节。
4. 基于规则的分词：有些语言具有明确的分词规则，可以根据这些规则将文本分解为单词。例如，日语中的分词可以基于汉字辞书或者语法规则进行。
5. 基于统计的分词：这种方式使用统计模型来判断文本中的分词边界。例如，隐马尔可夫模型（Hidden Markov Model，HMM）可以通过训练来预测最可能的分词边界。

需要注意的是，不同的大模型LLMs可能采用不同的分词方式，甚至在同一个模型中，可以根据任务和语料库的需求进行调整。这些分词方式的选择会对模型的性能和效果产生影响，因此需要根据具体情况进行选择和调整。

##### 2 介绍一下不同大模型LLMs的分词方式的区别？

不同的大模型LLMs（Language Models）在分词方式上可能存在一些区别。以下是一些常见的分词方式及其区别：

1. 基于规则的分词：这种分词方式使用预定义的规则和模式来切分文本。例如，可以使用空格、标点符号或特定的字符来确定词语的边界。这种方法简单直接，但对于复杂的语言和文本结构可能不够准确。
2. 基于统计的分词：这种分词方式使用统计模型来确定词语的边界。通常会使用大量的标注数据来训练模型，并根据词语的频率和上下文来进行切分。这种方法相对准确，但对于未见过的词语或特定领域的术语可能不够准确。
3. 基于深度学习的分词：这种分词方式使用深度学习模型，如循环神经网络（RNN）或Transformer模型，来进行分词。这些模型可以学习文本的上下文信息，并根据语义和语法规则来进行切分。这种方法可以处理复杂的语言结构和未见过的词语，但需要大量的训练数据和计算资源。
4. 基于预训练模型的分词：最近的研究表明，使用预训练的语言模型，如BERT、GPT等，可以在分词任务上取得很好的效果。这些模型在大规模的文本数据上进行预训练，并能够学习到丰富的语言表示。在具体的分词任务中，可以通过在预训练模型上进行微调来进行分词。这种方法具有较高的准确性和泛化能力。

需要注意的是，不同的大模型LLMs可能在分词方式上有所差异，具体的实现和效果可能因模型的结构、训练数据和任务设置而有所不同。选择适合特定任务和语言的分词方式是很重要的。
