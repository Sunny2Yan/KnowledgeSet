# Flash Attention

![](/imgs/llm/flash_att/f_a_1.png)

## 1. Standard Attention 
ç»™å®šè¾“å…¥åºåˆ— $Q, K, V \in \mathbb{R}^{N \times d}$ï¼Œå…¶ä¸­ $ğ‘$æ˜¯åºåˆ—é•¿åº¦ã€$ğ‘‘$ æ˜¯ head dimensionï¼Œé€šè¿‡ä¸‹é¢å…¬å¼è®¡ç®— attention è¾“å‡º $O \in \mathbb{R}^{N \times d}$ï¼š
$$
\begin{align}
S &= QK^{T} \in \mathbb{R}^{N \times N} \\
P &= softmax(S) \mathbb{R}^{N \times N} \\
O &= PV \in \mathbb{R}^{N \times d} \\
\end{align}
$$

![](/imgs/llm/flash_att/f_a_2.png)

ç¼ºç‚¹ï¼šç”±äº SRAM ç©ºé—´è¾ƒå°ï¼Œè¿™æ ·ç›´æ¥å¤§é‡çš„è¯»å†™å¯¼è‡´ Attention è¿ç®—é€Ÿåº¦è¾ƒæ…¢ï¼Œè€Œä¸”ä¼šæœ‰å†…å­˜ç¢ç‰‡åŒ–é—®é¢˜ã€‚

## 2. Flash Attention V1
