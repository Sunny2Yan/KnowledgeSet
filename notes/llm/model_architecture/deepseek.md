# DeepSeek æ¨¡å‹ç»“æ„

## 1. deepseek V3

### 1.1 å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Latent Attentionï¼‰

![](/imgs/llm/deepseek/mla.png)

å¯¹äº input hidden $h_t$ï¼ŒMLAè®¡ç®—å¦‚ä¸‹ï¼š
$$
\begin{array}{r|l}
\begin{aligned}
    Latent ~ c_t^{KV} &= W^{DKV} h_t \\
    [k_{t,1}^C, k_{t,2}^C, \cdots, k_{t,n_h}^C] = k_t^C &= W^{UK} c_t^{KV} \\
    k_t^R &= \mathrm{RoPE}(W^{KR} h_t) \\
    k_{t,i} &= [k_{t,i}^C, k_t^R] \\
    [v_{t,1}^C, v_{t,2}^C, \cdots, v_{t,n_h}^C] = v_t^C &= W^{UV} c_t^{KV}
\end{aligned} 
&
\begin{aligned}
    Latent ~ c_t^{Q} &= W^{DQ} h_t \\
    [q_{t,1}^C, q_{t,2}^C, \cdots, q_{t,n_h}^C] = q_t^C &= W^{UQ} c_t^{Q} \\
    [q_{t,1}^R, q_{t,2}^R, \cdots, q_{t,n_h}^R] = q_t^R &= \mathrm{RoPE}(W^{QR} c_t^Q) \\
    q_{t,i} &= [q_{t,i}^C, q_{t,i}^R] \\
\end{aligned}
\end{array}
$$

å…¶ä¸­ï¼Œ$h_t\in \mathbb{R}^d$ è¡¨ç¤ºè¾“å…¥çš„ç¬¬ $t$ ä¸ªtokenï¼Œ$n_h$ è¡¨ç¤º attention head æ•°, $d_h$ è¡¨ç¤ºæ¯ä¸ªhead attentionçš„ç»´åº¦ã€‚
- $W^{DKV} \in \mathbb{R}^{d_c \times d}, W^{UV} \in \mathbb{R}^{n_hd_h \times d_c}$ åˆ†åˆ«è¡¨ç¤ºä¸‹é‡‡æ ·å’Œä¸Šé‡‡æ ·çŸ©é˜µ($d_c \ll n_hd_h$)
- $W^{KR} \in \mathbb{R}^{d_h^R \times d}$ ç”¨äºç”Ÿæˆæºå¸¦æ—‹è½¬ä½ç½®ç¼–ç çš„è§£è€¦é”®
- $W^{DQ} \in \mathbb{R}^{d'_c}, W^{UQ} \in \mathbb{R}^{n_hd_h \times ğ‘‘'_c}$ åˆ†åˆ«è¡¨ç¤ºä¸‹é‡‡æ ·å’Œä¸Šé‡‡æ ·çŸ©é˜µï¼ˆ$d'_c \ll n_hd_h$ï¼‰
- $W^{QR} \in \mathbb{R}^{d_h^R n_h \times d'_c}$ ç”¨äºç”Ÿæˆæºå¸¦æ—‹è½¬ä½ç½®ç¼–ç çš„è§£è€¦æŸ¥è¯¢çŸ©é˜µ

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨ KV cacheç¼“å­˜æ—¶åªéœ€è¦ç¼“å­˜ $c_t^{KV}$ å’Œ $k_t^R$ï¼Œå¤§å¤§é™ä½ç¼“å­˜ç©ºé—´ã€‚


$$
\begin{aligned}
o_{t,i} &= \sum_{j=1}^{t} \mathrm{Softmax}_j \frac{(q_{t,i}^T k_{j,i})}{\sqrt{d_h + d_h^R}} v_{j,i}^C \\
u_t &= W^{O} [o_{t,1}, o_{t,2}, \cdots, o_{t,n_h}]
\end{aligned}
$$

```python
import math
import torch
import torch.nn as nn

class MLA(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.dim = args.dim
        self.n_heads = args.n_heads
        self.n_local_heads = args.n_heads  # 16
        self.qk_nope_head_dim = args.qk_nope_head_dim
        self.qk_rope_head_dim = args.qk_rope_head_dim
        self.qk_head_dim = args.qk_nope_head_dim + args.qk_rope_head_dim  # 128 + 64
        self.v_head_dim = args.v_head_dim

        self.wq = nn.Linear(self.dim, self.n_heads * self.qk_head_dim)
            
        self.wkv_a = nn.Linear(self.dim, self.kv_lora_rank + self.qk_rope_head_dim)
        self.kv_norm = RMSNorm(self.kv_lora_rank)
        self.wkv_b = ColumnParallelLinear(self.kv_lora_rank, self.n_heads * (self.qk_nope_head_dim + self.v_head_dim))
        self.wo = RowParallelLinear(self.n_heads * self.v_head_dim, self.dim)
        self.softmax_scale = self.qk_head_dim ** -0.5
        if args.max_seq_len > args.original_seq_len:
            mscale = 0.1 * args.mscale * math.log(args.rope_factor) + 1.0
            self.softmax_scale = self.softmax_scale * mscale * mscale

        self.register_buffer("k_cache", torch.zeros(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.qk_head_dim), persistent=False)
        self.register_buffer("v_cache", torch.zeros(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.v_head_dim), persistent=False)

    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):
        bsz, seqlen, _ = x.size()
        end_pos = start_pos + seqlen
        q = self.wq(x)  # compression q: [bs, seq, 2048] -> [bs, seq, 16*512]
            
        q = q.view(bsz, seqlen, self.n_local_heads, self.qk_head_dim)  # [bs, seq, 16, 128+64]
        q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)
        q_pe = apply_rotary_emb(q_pe, freqs_cis)

        kv = self.wkv_a(x)  # compression kv: [bs, seq, 2048] -> [bs, seq, 512+64]
        kv, k_pe = torch.split(kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)
        k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis)

        q = torch.cat([q_nope, q_pe], dim=-1)
        kv = self.wkv_b(self.kv_norm(kv))
        kv = kv.view(bsz, seqlen, self.n_local_heads, self.qk_nope_head_dim + self.v_head_dim)
        k_nope, v = torch.split(kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)
        k = torch.cat([k_nope, k_pe.expand(-1, -1, self.n_local_heads, -1)], dim=-1)
        self.k_cache[:bsz, start_pos:end_pos] = k
        self.v_cache[:bsz, start_pos:end_pos] = v
        scores = torch.einsum("bshd,bthd->bsht", q, self.k_cache[:bsz, :end_pos]) * self.softmax_scale
        
        if mask is not None:
            scores += mask.unsqueeze(1)
        scores = scores.softmax(dim=-1, dtype=torch.float32).type_as(x)
        x = torch.einsum("bsht,bthd->bshd", scores, self.v_cache[:bsz, :end_pos])
        
        x = self.wo(x.flatten(2))
        return x
```


### 1.2 deepseek MoE æ¶æ„
ä»¤ $u_t$ è¡¨ç¤º FFN è¾“å…¥çš„ç¬¬ $t$ ä¸ªtokenï¼Œè®¡ç®— FFN çš„è¾“å‡º $h_t^{'}$ å¦‚ä¸‹ï¼š

$$
\begin{aligned}
    h_t^{'} &= u_t + \sum_{i=1}^{N_s} \mathrm{FFN}_i^{(s)}(u_t) + \sum_{i=1}^{N_r} g_{i,t} \mathrm{FFN}_i^{(r)}(u_t) \\
    g_{i,t} &= \frac{g_{i,t}^{'}}{\sum_{j=1}^{N_r} g_{j,t}^{'}} \\
    g_{j,t}^{'} &=  
    \begin{cases}
    s_{i,t}  & s_{i,t} \in \mathrm{Topk}(\{s_{i,t}|1\leq j \leq N_r\}, K_r) \\
    0 & otherwise
    \end{cases} \\
    s_{i,t} &= \mathrm{Sigmoid}(u_t^T e_i)
\end{aligned}
$$

- $N_s, N_r$ åˆ†åˆ«è¡¨ç¤º shared experts å’Œ routed experts çš„æ•°é‡
- $\mathrm{FFN}_i^{(s)}(\cdot), \mathrm{FFN}_i^{(r)}(\cdot)$ åˆ†åˆ«è¡¨ç¤ºç¬¬ $i$ ä¸ª shared experts å’Œ routed experts
- $K_r$ è¡¨ç¤º active routed experts çš„æ•°é‡
- $g_{i,t}$ è¡¨ç¤ºç¬¬ $i$ ä¸ªä¸“å®¶çš„æƒé‡ç³»æ•°
- $s_{i,t}$ è¡¨ç¤º token ä¸ä¸“å®¶çš„ç›¸å…³åº¦
- $e_i$ è¡¨ç¤ºç¬¬ $i$ ä¸ªä¸“å®¶çš„ç‰¹å¾å‘é‡
- $\mathrm{Topk}(\cdot, K)$ è¡¨ç¤ºç¬¬ $t$ ä¸ª token ä¸æ‰€æœ‰ routed exports è®¡ç®—å¾—åˆ°çš„ç›¸å…³åº¦å¾—åˆ†æœ€é«˜çš„ $K$ ä¸ªå€¼

### 1.3 æ— è¾…åŠ©æŸå¤±è´Ÿè½½å‡è¡¡ï¼ˆAuxiliary-Loss-Free Load Balancingï¼‰

