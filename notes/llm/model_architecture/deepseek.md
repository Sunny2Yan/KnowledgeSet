# DeepSeek æ¨¡å‹ç»“æ„

## 1. deepseek V3

### 1.1 å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Latent Attentionï¼‰

![](/imgs/llm/deepseek/mla.png)

å¯¹äº input hidden $h_t$ï¼ŒMLAè®¡ç®—å¦‚ä¸‹ï¼š
$$
\begin{array}{r|l}
\begin{aligned}
    Latent ~ c_t^{KV} &= W^{DKV} h_t \\
    [k_{t,1}^C, k_{t,2}^C, \cdots, k_{t,n_h}^C] = k_t^C &= W^{UK} c_t^{KV} \\
    k_t^R &= \mathrm{RoPE}(W^{KR} h_t) \\
    k_{t,i} &= [k_{t,i}^C, k_t^R] \\
    [v_{t,1}^C, v_{t,2}^C, \cdots, v_{t,n_h}^C] = v_t^C &= W^{UV} c_t^{KV}
\end{aligned} 
&
\begin{aligned}
    Latent ~ c_t^{Q} &= W^{DQ} h_t \\
    [q_{t,1}^C, q_{t,2}^C, \cdots, q_{t,n_h}^C] = q_t^C &= W^{UQ} c_t^{Q} \\
    [q_{t,1}^R, q_{t,2}^R, \cdots, q_{t,n_h}^R] = q_t^R &= \mathrm{RoPE}(W^{QR} c_t^Q) \\
    q_{t,i} &= [q_{t,i}^C, q_{t,i}^R] \\
\end{aligned}
\end{array}
$$

å…¶ä¸­ï¼Œ$h_t\in \mathbb{R}^d$ è¡¨ç¤ºè¾“å…¥çš„ç¬¬ $t$ ä¸ªtokenï¼Œ$n_h$ è¡¨ç¤º attention head æ•°, $d_h$ è¡¨ç¤ºæ¯ä¸ªhead attentionçš„ç»´åº¦ã€‚
- $W^{DKV} \in \mathbb{R}^{d_c \times d}, W^{UV} \in \mathbb{R}^{n_hd_h \times d_c}$ åˆ†åˆ«è¡¨ç¤ºä¸‹é‡‡æ ·å’Œä¸Šé‡‡æ ·çŸ©é˜µ($d_c \ll n_hd_h$)
- $W^{KR} \in \mathbb{R}^{d_h^R \times d}$ ç”¨äºç”Ÿæˆæºå¸¦æ—‹è½¬ä½ç½®ç¼–ç çš„è§£è€¦é”®
- $W^{DQ} \in \mathbb{R}^{d'_c}, W^{UQ} \in \mathbb{R}^{n_hd_h \times ğ‘‘'_c}$ åˆ†åˆ«è¡¨ç¤ºä¸‹é‡‡æ ·å’Œä¸Šé‡‡æ ·çŸ©é˜µï¼ˆ$d'_c \ll n_hd_h$ï¼‰
- $W^{QR} \in \mathbb{R}^{d_h^R n_h \times d'_c}$ ç”¨äºç”Ÿæˆæºå¸¦æ—‹è½¬ä½ç½®ç¼–ç çš„è§£è€¦æŸ¥è¯¢çŸ©é˜µ

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨ KV cacheç¼“å­˜æ—¶åªéœ€è¦ç¼“å­˜ $c_t^{KV}$ å’Œ $k_t^R$ï¼Œå¤§å¤§é™ä½ç¼“å­˜ç©ºé—´ã€‚


$$
\begin{aligned}
o_{t,i} &= \sum_{j=1}^{t} \mathrm{Softmax}_j \frac{(q_{t,i}^T k_{j,i})}{\sqrt{d_h + d_h^R}} v_{j,i}^C \\
u_t &= W^{O} [o_{t,1}, o_{t,2}, \cdots, o_{t,n_h}]
\end{aligned}
$$

```python
import math
import torch
import torch.nn as nn

class MLA(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.dim = args.dim
        self.n_heads = args.n_heads
        self.n_local_heads = args.n_heads  # 16
        self.qk_nope_head_dim = args.qk_nope_head_dim
        self.qk_rope_head_dim = args.qk_rope_head_dim
        self.qk_head_dim = args.qk_nope_head_dim + args.qk_rope_head_dim  # 128 + 64
        self.v_head_dim = args.v_head_dim

        self.wq = nn.Linear(self.dim, self.n_heads * self.qk_head_dim)
            
        self.wkv_a = nn.Linear(self.dim, self.kv_lora_rank + self.qk_rope_head_dim)
        self.kv_norm = RMSNorm(self.kv_lora_rank)
        self.wkv_b = ColumnParallelLinear(self.kv_lora_rank, self.n_heads * (self.qk_nope_head_dim + self.v_head_dim))
        self.wo = RowParallelLinear(self.n_heads * self.v_head_dim, self.dim)
        self.softmax_scale = self.qk_head_dim ** -0.5
        if args.max_seq_len > args.original_seq_len:
            mscale = 0.1 * args.mscale * math.log(args.rope_factor) + 1.0
            self.softmax_scale = self.softmax_scale * mscale * mscale

        self.register_buffer("k_cache", torch.zeros(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.qk_head_dim), persistent=False)
        self.register_buffer("v_cache", torch.zeros(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.v_head_dim), persistent=False)

    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):
        bsz, seqlen, _ = x.size()
        end_pos = start_pos + seqlen
        q = self.wq(x)  # compression q: [bs, seq, 2048] -> [bs, seq, 16*512]
            
        q = q.view(bsz, seqlen, self.n_local_heads, self.qk_head_dim)  # [bs, seq, 16, 128+64]
        q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)
        q_pe = apply_rotary_emb(q_pe, freqs_cis)

        kv = self.wkv_a(x)  # compression kv: [bs, seq, 2048] -> [bs, seq, 512+64]
        kv, k_pe = torch.split(kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)
        k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis)

        q = torch.cat([q_nope, q_pe], dim=-1)
        kv = self.wkv_b(self.kv_norm(kv))
        kv = kv.view(bsz, seqlen, self.n_local_heads, self.qk_nope_head_dim + self.v_head_dim)
        k_nope, v = torch.split(kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)
        k = torch.cat([k_nope, k_pe.expand(-1, -1, self.n_local_heads, -1)], dim=-1)
        self.k_cache[:bsz, start_pos:end_pos] = k
        self.v_cache[:bsz, start_pos:end_pos] = v
        scores = torch.einsum("bshd,bthd->bsht", q, self.k_cache[:bsz, :end_pos]) * self.softmax_scale
        
        if mask is not None:
            scores += mask.unsqueeze(1)
        scores = scores.softmax(dim=-1, dtype=torch.float32).type_as(x)
        x = torch.einsum("bsht,bthd->bshd", scores, self.v_cache[:bsz, :end_pos])
        
        x = self.wo(x.flatten(2))
        return x
```


### 1.2 deepseek MoE æ¶æ„
ä»¤ $u_t$ è¡¨ç¤º FFN è¾“å…¥çš„ç¬¬ $t$ ä¸ªtokenï¼Œè®¡ç®— FFN çš„è¾“å‡º $h_t^{'}$ å¦‚ä¸‹ï¼š

$$
\begin{aligned}
    h_t^{'} &= u_t + \sum_{i=1}^{N_s} \mathrm{FFN}_i^{(s)}(u_t) + \sum_{i=1}^{N_r} g_{i,t} \mathrm{FFN}_i^{(r)}(u_t) \\
    g_{i,t} &= \frac{g_{i,t}^{'}}{\sum_{j=1}^{N_r} g_{j,t}^{'}} \\
    g_{j,t}^{'} &=  
    \begin{cases}
    s_{i,t}  & s_{i,t} \in \mathrm{Topk}(\{s_{i,t}|1\leq j \leq N_r\}, K_r) \\
    0 & otherwise
    \end{cases} \\
    s_{i,t} &= \mathrm{Sigmoid}(u_t^T e_i)
\end{aligned}
$$

- $N_s, N_r$ åˆ†åˆ«è¡¨ç¤º shared experts å’Œ routed experts çš„æ•°é‡
- $\mathrm{FFN}_i^{(s)}(\cdot), \mathrm{FFN}_i^{(r)}(\cdot)$ åˆ†åˆ«è¡¨ç¤ºç¬¬ $i$ ä¸ª shared experts å’Œ routed experts
- $K_r$ è¡¨ç¤º active routed experts çš„æ•°é‡
- $g_{i,t}$ è¡¨ç¤ºç¬¬ $i$ ä¸ªä¸“å®¶çš„æƒé‡ç³»æ•°
- $s_{i,t}$ è¡¨ç¤º token ä¸ä¸“å®¶çš„äº²å’Œåº¦
- $e_i$ è¡¨ç¤ºç¬¬ $i$ ä¸ªä¸“å®¶çš„ç‰¹å¾å‘é‡
- $\mathrm{Topk}(\cdot, K)$ è¡¨ç¤ºç¬¬ $t$ ä¸ª token ä¸æ‰€æœ‰ routed exports è®¡ç®—å¾—åˆ°çš„ç›¸å…³åº¦å¾—åˆ†æœ€é«˜çš„ $K$ ä¸ªå€¼

### 1.3 æ— è¾…åŠ©æŸå¤±è´Ÿè½½å‡è¡¡ï¼ˆAuxiliary-Loss-Free Load Balancingï¼‰

è§£å†³ MoE è´Ÿè½½å‡è¡¡é—®é¢˜ï¼Œä¼ ç»Ÿçš„è§£å†³æ–¹æ³•æ˜¯ä¾èµ–äºè¾…åŠ©æŸå¤±ï¼Œä½†è¿‡å¤§çš„æŸå¤±ä¼šé™ä½æ¨¡å‹æ€§èƒ½ï¼Œæœ¬æ–¹æ³•ä¸ºæ¯ä¸ªä¸“å®¶å¼•å…¥äº†ä¸€ä¸ªåç½®é¡¹ï¼Œä½œä¸ºç›¸åº”çš„äº²å’Œåº¦å¾—åˆ†ï¼Œä»¥ç¡®å®š top-K è·¯ç”±ï¼š
$$
g_{i,t}^{'} = 
\begin{cases}
s_{i,t}  & s_{i,t}+b_i \in \mathrm{Topk}(\{s_{i,t}+b_i|1\leq j \leq N_r\}, K_r) \\
0 & otherwise
\end{cases}
$$
åç½®é¡¹ä»…ç”¨äºè·¯ç”±é€‰æ‹©ï¼Œgating value ä»åŸºäºåŸå§‹äº²å’Œåº¦å¾—åˆ†ã€‚
å³ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®æ—¶ç›‘æ§æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­æ‰€æœ‰æ‰¹æ¬¡çš„ä¸“å®¶è´Ÿè½½åˆ†å¸ƒï¼Œæ­¥éª¤ç»“æŸæ—¶ï¼Œè´Ÿè½½è¿‡é«˜çš„ä¸“å®¶åç½®é¡¹ä¼šå‡å°‘ $\gamma$ï¼Œè´Ÿè½½ä¸è¶³çš„ä¸“å®¶åç½®é¡¹å¢åŠ  $\gamma$ã€‚

### 1.4 åºåˆ—çº§è¾…åŠ©æŸå¤±è¡¥å……æœºåˆ¶

### 1.5 Node-Limited Routing
ç›®çš„ï¼šåœ¨è®­ç»ƒæœŸé—´é™åˆ¶é€šä¿¡æˆæœ¬ã€‚å³ï¼Œæ ¹æ®åˆ†å¸ƒåœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„ä¸“å®¶çš„æœ€é«˜äº²å’Œåº¦åˆ†æ•°ä¹‹å’Œ $\frac{K_r}{M}$ æ¥é€‰æ‹©ï¼Œç¡®ä¿æ¯ä¸ªtokenå°†è¢«å‘é€åˆ°æœ€å¤š M ä¸ª GPU nodeã€‚

## 2. å¤štokené¢„æµ‹ï¼ˆMulti-Token Predictionï¼ŒNTPï¼‰
![](/imgs/llm/deepseek/mtp.png)

ç›®çš„ï¼šé™ä½ training cast -> æ–¹æ³•ï¼šspeculative decoding -> æé«˜å®ƒçš„æ€§èƒ½ã€‚
æ–¹æ³•ï¼šä½¿ç”¨ D sequential modules æ¥é¢„æµ‹ D ä¸ªé¢å¤–çš„ tokensã€‚MTP æ¨¡å—åŒ…å«ä¸ä¸»æ¨¡å‹å…±äº«çš„ embedding å±‚ $\mathrm{Emb}(\cdot)$ã€è¾“å‡ºå¤´ $\mathrm{OutHead}(\cdot)$ å’Œ 
Transformer å— $\mathrm{TRM}_k(\cdot)$ã€æŠ•å½±çŸ©é˜µ $M_k \in \mathbb{R}^{d \times 2d}$ã€‚
 
è®°ç¬¬ $i$ ä¸ª token åœ¨ç¬¬ $k-1$ ä¸ªé¢„æµ‹æ·±åº¦ä¸º $h_i^{k-1} \in \mathbb{R}^d$ï¼ˆ$k=1$ è¡¨ç¤º main modelï¼‰ï¼Œåˆ™å¯¹äºç¬¬ $i$ ä¸ªè¾“å…¥token $t_i$ï¼Œåœ¨ç¬¬ $k$ ä¸ªé¢„æµ‹æ·±åº¦ï¼š
$$
\begin{aligned}
h_i^{'k} &= M_k [\mathrm{RMSNorm}(h_i^{k-1}); \mathrm{RMSNorm}(\mathrm{Emb}(t_{i+k}))] \\
h_{i:T-k}^k &= \mathrm{TRM}_K(h_{i:T-k}^{'k}) \\
p_{i+k+1}^k &= \mathrm{Softmax}(\mathrm{OutHead}(h_i^k))
\end{aligned}
$$

å…¶ä¸­ï¼Œ$T$ è¡¨ç¤º sequence lengthã€‚

**å¯¹äºè®­ç»ƒï¼š**

$$
\begin{aligned}
\mathcal{L}_{MTP}^{k} &= \mathrm{CrossEntropy}(p_{2+k:T+1}^k, t_{2+k:T+1}) = -\frac{1}{T} \sum_{i=2+k}^{T+1} \log p_i^t [t_i] \\
\mathcal{L}_{MTP} &= \frac{\lambda}{D} \sum_{k=1}^{D} \mathcal{L}_{MTP}^{k}
\end{aligned}
$$

**å¯¹äºæ¨ç†ï¼š**

MTP ç­–ç•¥ä¸»è¦æ˜¯ä¸ºäº†æé«˜ main model çš„æ€§èƒ½ï¼Œå› æ­¤åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥ç›´æ¥ä¸¢å¼ƒ MTP æ¨¡å—ï¼Œç‹¬ç«‹è¿è¡Œ main modelã€‚


```python
import torch
import torch.nn as nn

class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-8):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    def _norm(self, x: torch.Tensor):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
    def forward(self, x: torch.Tensor):
        return self.weight * self._norm(x.float()).type_as(x)

class share_embedding(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.embed_tokens = nn.Embedding(
                config.vocab_size, config.hidden_size
            )
    def forward(self,input_ids):
        return self.embed_tokens(input_ids)
        
class share_output_head(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)
    def forward(self,transformer_hidden):
        return self.lm_head(transformer_hidden)
        
class transformer_block(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.mla = MLA(config)
        self.moe = MOE(config)
        self.pre_RMSNorm = RMSNorm(config)
        self.post_RMSNorm = RMSNorm(config)
        
    def forward(self, input_hidden):
        out_logits = self.pre_RMSNorm(input_hidden)
        out_logits = self.mla(out_logits)
        out_logits = self.post_RMSNorm(out_logits)        
        moe_input = input_hidden+out_logits
        moe_output = self.moe(moe_input)
        block_output = moe_output+moe_input
        return block_output
        
class MTP(nn.Module):
    def __init__(self,config):
        super().__init__()
        self.RMSNorm_right = RMSNorm(config)
        self.RMSNorm_left = RMSNorm(config)
        self.transformer = transformer_block(config)
        self.proj = nn.Linear(2*config.hidden_size,config.hidden_size)
    def forward(self,last_block_out,input_ids_truncated,share_embeding,share_lm_head):
        last_norm_out = self.RMSNorm_right(last_block_out)
        embeding_trunc = share_embeding(input_ids_truncated)
        concat_input = torch.cat((last_norm_out, embeding_trunc), dim=-1)
        proj_out = self.proj(concat_input)
        trans_out_logits = self.transformer(proj_out)
        return trans_out_logits
        
class MTP_and_deepseek(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.share_embedding = share_embedding(config)
        self.share_lm_head = share_output_head(config)
        self.loss = nn.CrossEntropyLoss(ignore = config.pad_token_id)
        self.Model_trans_blocks = nn.ModuleList(
            [
                transformer_block(config, layer_idx)
                for layer_idx in range(config.num_hidden_layers)
            ]
        )
        self.MTP_trans_blocks = nn.ModuleList(
            [
                transformer_block(config, layer_idx)
                for layer_idx in range(config.num_MTP_layers)
            ]
        )
        self.config = config
        self.alpha_list = config.alpha_list  # å¯¹äºæ¯ä¸€ä¸ªMTP lossçš„åŠ æƒåˆ—è¡¨
        
   def forward(self, input_ids):
       # input_ids: [bos,tok1,tok2......last_tok]
       # labels_origin: [tok1,tok2,....,last_tok,eos/pad]
       # labels_MTP1: [tok2,.....last_tok,eos,pad]
       # labels_MTP2: [tok3,.....last_tok,eos,pad,pad]
       embeding_logits = self.share_embedding(input_ids)
       deepseek_hidden = embeding_logits
       for index,trans_block in enumerate(self.Model_trans_blocks):
           deepseek_hidden = trans_block(deepseek_hidden)
       deepseek_logits = self.share_lm_head(deepseek_hidden)
       labels = torch.cat([input_ids[:, 1:], 
                           torch.full((input_ids.size(0), 1), self.config.pad_token_id)], dim=1)
       Main_loss = self.loss(deepseek_logits,labels)
       
       last_mtp_out = deepseek_hidden
       for ind, MTP in enumerate(self.MTP_trans_blocks):
           input_ids_trunc = torch.cat([input_ids[:, ind + 1 :], torch.full((input_ids.size(0), ind + 1), self.config.pad_token_id)], dim=1, )
           mtp_out = MTP(last_mtp_out, input_ids_trunc,self.share_embedding)
           mtp_logits = self.share_lm_head(mtp_out)
           last_mtp_out = mtp_out
           labels_trunc = torch.cat([input_ids_trunc[:, 1:], torch.full((input_ids.size(0), 1), config.pad_token_id)], dim=1)
           mtp_loss = self.loss(mtp_logits,labels_trunc)
           alpha = self.alpha_list[ind]
           Main_loss +=  alpha*mtp_loss
       return Main_loss
```
